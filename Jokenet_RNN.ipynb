{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JokeNet\n",
    "This notebook outlines a Recurrent Neural Net using LSTM cells to make jokes.\n",
    "\n",
    "For a comprehensive intro to using Recurrent Neural Nets and a few fun examples, look at this blog post by Andrei Karpathy - Tesla's AI lead. https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "The training data is an assortment from reddit, wocka.com and stupidstuff.org.\n",
    "\n",
    "It's also worthy to note that this code started from the [tensorflow tutorial on RNN's](https://www.tensorflow.org/tutorials/recurrent) and is based off of the code in tf_models.\n",
    "\n",
    "For further information about the algorithm and its implementation, see here:\n",
    "> (Zaremba, et. al.) Recurrent Neural Network Regularization\n",
    "> http://arxiv.org/abs/1409.2329\n",
    "\n",
    "\n",
    "Disclaimer: these jokes are scraped from the internet and so some may be offensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import dependencies to train our RNN\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import util\n",
    "\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Re)Scrape the Dataset\n",
    "\n",
    "Scraping the dataset occurs in the `jokes_dataset` folder. Re-run the scraping with the commands:\n",
    "```\n",
    "python reddit.py\n",
    "python wocka.py\n",
    "python stupidstuff.py\n",
    "```\n",
    "\n",
    "You can also get the entire dataset of jokes from here: https://github.com/taivop/joke-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_joke(id):\n",
    "    \"\"\"Download and parse a single joke.\"\"\"\n",
    "    \n",
    "    re_category_rating = re.compile(r\"\\s*Category: (.*[A-z])\\s*Rating: (.*\\d)\\s*\")\n",
    "\n",
    "    url_base = \"http://stupidstuff.org/jokes/joke.htm?jokeid={}\"\n",
    "    response = requests.get(url_base.format(id))\n",
    "\n",
    "    tree = html.fromstring(response.content)\n",
    "    content = tree.xpath('//table[@bgcolor=\"#ffffff\" and @width=\"470\"]//table[@class=\"scroll\"]//td')[0]\n",
    "    category_rating_cells = content.xpath('//table[@bgcolor=\"#ffffff\"]//table[@class=\"bkline\"]//td/b[text()=\"Category: \"]/..')\n",
    "    #print(category_rating_cell)\n",
    "    \n",
    "    # all html nodes in content, but not plaintext\n",
    "    crap = content.xpath('./child::node()[not(self::text()) and not(self::br)]') \n",
    "    \n",
    "    for node in crap:\n",
    "        content.remove(node)\n",
    "\n",
    "    body_text = content.text_content().strip()\n",
    "    joke_body = body_text\n",
    "\n",
    "    cell_text = category_rating_cells[0].text_content()\n",
    "\n",
    "    match = re_category_rating.search(cell_text)\n",
    "    category = match.group(1)\n",
    "    rating = float(match.group(2))\n",
    "\n",
    "    return joke_body, category, rating\n",
    "\n",
    "\n",
    "def save_stupid_stuff_jokes():\n",
    "    \"\"\"Parse and save all jokes to stupidstuff.json file\"\"\"\n",
    "    jokes = []\n",
    "\n",
    "    save_frequency = 100 # save after every 100 IDs\n",
    "    max_id = 3773\n",
    "    for id in range(1, max_id+1): #19000\n",
    "        try:\n",
    "            body, category, rating = extract_joke(id)\n",
    "\n",
    "            joke = {\"id\": id, \"category\": category, \"body\": body, \"rating\": rating}\n",
    "            jokes.append(joke)\n",
    "            print(\"ID {} success: [{}]\".format(id, category))\n",
    "        except Exception as ex:\n",
    "            print(\"ID {} failed: \".format(id))\n",
    "            logging.error(ex)\n",
    "            raise ex\n",
    "\n",
    "        if id % save_frequency == 0 or id == max_id:\n",
    "            with open(\"stupidstuff.json\", \"w\") as f:\n",
    "                json.dump(jokes, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing functions\n",
    "\n",
    "The data is formatted into json objects with various IDs:\n",
    "    \n",
    "reddit_jokes.json\n",
    "```json\n",
    "{\n",
    "    \"title\": \"My boss said to me, \\\"you're the worst train driver ever. How many have you derailed this year?\\\"\",\n",
    "    \"body\": \"I said, \\\"I'm not sure; it's hard to keep track.\\\"\",\n",
    "    \"id\": \"5tyytx\",\n",
    "    \"score\": 3\n",
    "}\n",
    "```\n",
    "\n",
    "stupidstuff.json\n",
    "```json\n",
    "{\n",
    "    \"category\": \"Blonde Jokes\",\n",
    "    \"body\": \"A blonde is walking down the street with her blouse open, exposing one of her breasts. A nearby policeman approaches her and remarks, \\\"Ma'am, are you aware that I could cite you for indecent exposure?\\\" \\\"Why, officer?\\\" asks the blonde. \\\"Because your blouse is open and your breast is exposed.\\\" \\\"Oh my goodness,\\\" exclaims the blonde, \\\"I must have left my baby on the bus!\\\"\",\n",
    "    \"id\": 14,\n",
    "    \"rating\": 3.5\n",
    "}\n",
    "```\n",
    "\n",
    "wocka.json\n",
    "```json\n",
    "{\n",
    "    \"title\": \"Infants vs Adults\",\n",
    "    \"body\": \"Do infants enjoy infancy as much as adults enjoy adultery?\",\n",
    "    \"category\": \"One Liners\",\n",
    "    \"id\": 17\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "The data must be processed into training, testing and validation sets. It also must be shuffled around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import os\n",
    "\n",
    "\n",
    "def read_reddit(filename):\n",
    "    '''Parse reddit jokes which often include the title into the joke.'''\n",
    "    jokes = []\n",
    "\n",
    "    with open(\"rnn_data/reddit_jokes.json\",\"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    for joke in data:\n",
    "        jokes.append(joke[\"title\"] + \"\\n\" + joke[\"body\"])\n",
    "    \n",
    "    return jokes\n",
    "\n",
    "def read_other(filename):\n",
    "    '''Parse other jokes from stupidstuff or wocka'''\n",
    "    jokes = []\n",
    "\n",
    "    with open(filename,\"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    for joke in data:\n",
    "        jokes.append(joke[\"body\"])\n",
    "    \n",
    "    return jokes\n",
    "\n",
    "\n",
    "def parse_raw_data(data_path=\"\"):\n",
    "    \"\"\"Collate data objects and randomly shuffle into train, test, validate splits\"\"\"\n",
    "    \n",
    "    jokes = []\n",
    "\n",
    "    jokes += read_reddit(os.path.join(data_path, \"reddit_jokes.json\"))\n",
    "    jokes += read_other(os.path.join(data_path, \"wocka.json\"))\n",
    "    jokes += read_other(os.path.join(data_path, \"stupidstuff.json\"))\n",
    "    \n",
    "    # shuffle jokes randomly\n",
    "    shuffle(jokes)\n",
    "    \n",
    "    jokes_length = len(jokes)\n",
    "    \n",
    "    # Generate train, test and validate sets\n",
    "    train, test, validate = jokes[0:int(0.7*jokes_length)], \\\n",
    "                            jokes[int(0.7*jokes_length)+1:int(0.85*jokes_length)], \\\n",
    "                            jokes[int(0.85*jokes_length)+1:]\n",
    "\n",
    "    with open(\"rnn_data/train.txt\",\"w\") as f:\n",
    "        for line in train:\n",
    "            f.write(line + \"\\n\\n\")\n",
    "    \n",
    "    with open(\"rnn_data/test.txt\",\"w\") as f:\n",
    "        for line in test:\n",
    "            f.write(line + \"\\n\\n\")\n",
    "            \n",
    "    with open(\"rnn_data/validate.txt\",\"w\") as f:\n",
    "        for line in validate:\n",
    "            f.write(line + \"\\n\\n\")\n",
    "    \n",
    "\n",
    "# uncomment if you want to reshuffle the data or you're running this for the first time\n",
    "# parse_raw_data(data_path=\"rnn_data\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further processing\n",
    "\n",
    "The data is then turned into lists of numbers representing the words. These numbers can then be trained on by the RNN.\n",
    "\n",
    "The data has a few symbols that should be considered. After the end of a joke and `<eos>` tag is added to indicate the end of sentence. This helps us re-generate new jokes.\n",
    "\n",
    "Furthermore, the words have to be turned into numbers so that we can compute the weights and node values. Here we define some helper functions that do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import sys\n",
    "\n",
    "\n",
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\\n\", \"<eos>\").replace(\"\\r\", \"\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename, maxlen=-1):\n",
    "    data = _read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words[0:maxlen], range(len(words[0:maxlen]))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "# Our RNN will have a limited vocabulary\n",
    "def get_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "    \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "\n",
    "    Reads PTB text files, converts strings to integer ids,\n",
    "    and performs mini-batching of the inputs.\n",
    "\n",
    "    Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "\n",
    "    Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "    \"\"\"\n",
    "\n",
    "    train_path = os.path.join(data_path, \"train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"test.txt\")\n",
    "    test_path = os.path.join(data_path, \"validate.txt\")\n",
    "\n",
    "    word_to_id = _build_vocab(train_path, 9999)\n",
    "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "# Training is usually done by breaking up the input data into smaller pieces called batches.\n",
    "# This makes training faster and allows better insight into the performance of the algorithm.\n",
    "# Here we create tensors (3d Matrices) that serve as our batches.\n",
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "    \"\"\"Iterate on the raw PTB data.\n",
    "\n",
    "    This chunks up raw_data into batches of examples and returns Tensors that\n",
    "    are drawn from these batches.\n",
    "\n",
    "    Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    name: the name of this operation (optional).\n",
    "\n",
    "    Returns:\n",
    "    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "    of the tuple is the same data time-shifted to the right by one.\n",
    "\n",
    "    Raises:\n",
    "    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "        raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    assertion = tf.assert_positive(\n",
    "        epoch_size,\n",
    "        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    with tf.control_dependencies([assertion]):\n",
    "        epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = tf.strided_slice(data, [0, i * num_steps],\n",
    "                         [batch_size, (i + 1) * num_steps])\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = tf.strided_slice(data, [0, i * num_steps + 1],\n",
    "                         [batch_size, (i + 1) * num_steps + 1])\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the RNN Model\n",
    "\n",
    "Tensorflow wraps up the math for us but we have to look at a multitude of `tf.this.that.theother`. Don't let this scare you though - in fact the entirety of a neural network could be written in [11 lines of python without tensorflow](https://iamtrask.github.io/2015/07/12/basic-python-network/). Tensorflow allows us to do fancier things faster - at least thats the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "flags.DEFINE_string('model', 'small',\n",
    "                    'A type of model. Possible options are: small, medium, large.'\n",
    "                    )\n",
    "flags.DEFINE_string('data_path', None,\n",
    "                    'Where the training/test data is stored.')\n",
    "flags.DEFINE_string('save_path', None, 'Model output directory.')\n",
    "flags.DEFINE_bool('use_fp16', False,\n",
    "                  'Train using 16-bit floats instead of 32bit floats')\n",
    "flags.DEFINE_integer('num_gpus', 1,\n",
    "                     'If larger than 1, Grappler AutoParallel optimizer will create multiple training replicas with each GPU running one replica.'\n",
    "                     )\n",
    "\n",
    "flags.DEFINE_string('rnn_mode', None,\n",
    "                    'The low level implementation of lstm cell: one of CUDNN, BASIC, and BLOCK, representing cudnn_lstm, basic_lstm, and lstm_block_cell classes.'\n",
    "                    )\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "BASIC = 'basic'\n",
    "CUDNN = 'cudnn'\n",
    "BLOCK = 'block'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Object\n",
    "The PTBInput Object serves as a nice way to serve tensors to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_type():\n",
    "    return tf.float32 #(tf.float16 if FLAGS.use_fp16 else tf.float32)\n",
    "\n",
    "\n",
    "class PTBInput(object):\n",
    "\n",
    "    \"\"\"The input data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        data,\n",
    "        name=None,\n",
    "        ):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = (len(data) // batch_size - 1) // num_steps\n",
    "        (self.input_data, self.targets) = ptb_producer(data,\n",
    "                batch_size, num_steps, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN Model\n",
    "\n",
    "We use a class structure so that we can have all methods and variables related to the RNN in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    \n",
    "    # init begins by setting up our RNN with the config file that we specified.\n",
    "    # It sets all the environment variables so we can begin learning\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_training,\n",
    "        config,\n",
    "        input_,):\n",
    "        self._is_training = is_training\n",
    "        self._input = input_\n",
    "        self._rnn_params = None\n",
    "        self._cell = None\n",
    "        self.batch_size = input_.batch_size\n",
    "        self.num_steps = input_.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [vocab_size,\n",
    "                    size], dtype=tf.float32) #data_type()\n",
    "            inputs = tf.nn.embedding_lookup(embedding,\n",
    "                    input_.input_data)\n",
    "            \n",
    "            \n",
    "        # Once it has set things up, it starts the training process\n",
    "        # First, it gets the inputs and creates a tf.nn.dropout layer - a layer of nodes\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "        \n",
    "        \n",
    "        # See build_rnn_graph method below\n",
    "        (output, state) = self._build_rnn_graph(inputs, config,\n",
    "                is_training)\n",
    "        \n",
    "        \n",
    "        # After creating all the layers, the RNN then using the softmax activation function\n",
    "        # to set the node values\n",
    "        softmax_w = tf.get_variable('softmax_w', [size, vocab_size],\n",
    "                                    dtype=tf.float32) #data_type()\n",
    "        softmax_b = tf.get_variable('softmax_b', [vocab_size],\n",
    "                                    dtype=tf.float32) #data_type()\n",
    "        \n",
    "        # The logits are similar to the probability for the output classes\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "\n",
    "        # Reshape logits to be a 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps,\n",
    "                            vocab_size])\n",
    "        \n",
    "        self._output_probs = tf.nn.softmax(logits)\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        # This tells us how good or bad our model is\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits, input_.targets,\n",
    "                tf.ones([self.batch_size, self.num_steps],\n",
    "                dtype=data_type()), average_across_timesteps=False,\n",
    "                average_across_batch=True)\n",
    "\n",
    "        # Is our model good our bad? Well this number shows us how far off\n",
    "        # our guesses were from reality. Thus we want to minimise this number.\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        \n",
    "        #Calculate the gradients so that we can minimise\n",
    "        (grads, _) = tf.clip_by_global_norm(tf.gradients(self._cost,\n",
    "                tvars), config.max_grad_norm)\n",
    "        \n",
    "        # Optimise our algorithm using Gradient Descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        \n",
    "        # Apply our new gradients to the RNN\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(tf.float32, shape=[],\n",
    "                name='new_learning_rate')\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "    \n",
    "    \n",
    "    # The RNN checks if there is some software installed and decides on which LSTM \n",
    "    # implementation to use.\n",
    "    def _build_rnn_graph(\n",
    "        self,\n",
    "        inputs,\n",
    "        config,\n",
    "        is_training,\n",
    "        ):\n",
    "        if config.rnn_mode == CUDNN:\n",
    "            return self._build_rnn_graph_cudnn(inputs, config,\n",
    "                    is_training)\n",
    "        else:\n",
    "            return self._build_rnn_graph_lstm(inputs, config,\n",
    "                    is_training)\n",
    "\n",
    "    def _build_rnn_graph_cudnn(\n",
    "        self,\n",
    "        inputs,\n",
    "        config,\n",
    "        is_training,\n",
    "        ):\n",
    "        \"\"\"Build the inference graph using CUDNN cell.\"\"\"\n",
    "\n",
    "        inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "        self._cell = \\\n",
    "            tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=config.num_layers,\n",
    "                num_units=config.hidden_size,\n",
    "                input_size=config.hidden_size, dropout=(1\n",
    "                - config.keep_prob if is_training else 0))\n",
    "        params_size_t = self._cell.params_size()\n",
    "        self._rnn_params = tf.get_variable('lstm_params',\n",
    "                initializer=tf.random_uniform([params_size_t],\n",
    "                -config.init_scale, config.init_scale),\n",
    "                validate_shape=False)\n",
    "        c = tf.zeros([config.num_layers, self.batch_size,\n",
    "                     config.hidden_size], tf.float32)\n",
    "        h = tf.zeros([config.num_layers, self.batch_size,\n",
    "                     config.hidden_size], tf.float32)\n",
    "        self._initial_state = (tf.contrib.rnn.LSTMStateTuple(h=h, c=c),\n",
    "                               )\n",
    "        (outputs, h, c) = self._cell(inputs, h, c, self._rnn_params,\n",
    "                is_training)\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
    "        return (outputs, (tf.contrib.rnn.LSTMStateTuple(h=h, c=c), ))\n",
    "\n",
    "    # A single LSTM cell\n",
    "    def _get_lstm_cell(self, config, is_training):\n",
    "        if config.rnn_mode == BASIC:\n",
    "            return tf.contrib.rnn.BasicLSTMCell(config.hidden_size,\n",
    "                    forget_bias=0.0, state_is_tuple=True,\n",
    "                    reuse=not is_training)\n",
    "        if config.rnn_mode == BLOCK:\n",
    "            return tf.contrib.rnn.LSTMBlockCell(config.hidden_size,\n",
    "                    forget_bias=0.0)\n",
    "        raise ValueError('rnn_mode %s not supported' % config.rnn_mode)\n",
    "\n",
    "    def _build_rnn_graph_lstm(\n",
    "        self,\n",
    "        inputs,\n",
    "        config,\n",
    "        is_training,\n",
    "        ):\n",
    "        \"\"\"Build the inference graph using canonical LSTM cells.\"\"\"\n",
    "\n",
    "    # Slightly better results can be obtained with forget gate biases\n",
    "    # initialized to 1 but the hyperparameters of the model would need to be\n",
    "    # different than reported in the paper.\n",
    "\n",
    "        def make_cell():\n",
    "            cell = self._get_lstm_cell(config, is_training)\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                        output_keep_prob=config.keep_prob)\n",
    "            return cell\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([make_cell() for _ in\n",
    "                range(config.num_layers)], state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(config.batch_size,\n",
    "                data_type())\n",
    "        state = self._initial_state\n",
    "\n",
    "    # Simplified version of tf.nn.static_rnn().\n",
    "    # This builds an unrolled LSTM for tutorial purposes only.\n",
    "    # In general, use tf.nn.static_rnn() or tf.nn.static_state_saving_rnn().\n",
    "    #\n",
    "    # The alternative version of the code below is:\n",
    "    #\n",
    "    # inputs = tf.unstack(inputs, num=self.num_steps, axis=1)\n",
    "    # outputs, state = tf.nn.static_rnn(cell, inputs,\n",
    "    #                                   initial_state=self._initial_state)\n",
    "\n",
    "        outputs = []\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for time_step in range(self.num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :],\n",
    "                        state)\n",
    "                outputs.append(cell_output)\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1,\n",
    "                            config.hidden_size])\n",
    "        return (output, state)\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "    def export_ops(self, name):\n",
    "        \"\"\"Exports training checkpoints.\"\"\"\n",
    "\n",
    "        self._name = name\n",
    "        ops = {util.with_prefix(self._name, 'cost'): self._cost}\n",
    "        if self._is_training:\n",
    "            ops.update(lr=self._lr, new_lr=self._new_lr,\n",
    "                       lr_update=self._lr_update)\n",
    "            if self._rnn_params:\n",
    "                ops.update(rnn_params=self._rnn_params)\n",
    "        for (name, op) in ops.items():\n",
    "            tf.add_to_collection(name, op)\n",
    "        self._initial_state_name = util.with_prefix(self._name,\n",
    "                'initial')\n",
    "        self._final_state_name = util.with_prefix(self._name, 'final')\n",
    "        util.export_state_tuples(self._initial_state,\n",
    "                                 self._initial_state_name)\n",
    "        util.export_state_tuples(self._final_state,\n",
    "                                 self._final_state_name)\n",
    "\n",
    "    def import_ops(self):\n",
    "        \"\"\"Imports training checkpoints.\"\"\"\n",
    "\n",
    "        if self._is_training:\n",
    "            self._train_op = tf.get_collection_ref('train_op')[0]\n",
    "            self._lr = tf.get_collection_ref('lr')[0]\n",
    "            self._new_lr = tf.get_collection_ref('new_lr')[0]\n",
    "            self._lr_update = tf.get_collection_ref('lr_update')[0]\n",
    "            rnn_params = tf.get_collection_ref('rnn_params')\n",
    "            if self._cell and rnn_params:\n",
    "                params_saveable = \\\n",
    "                    tf.contrib.cudnn_rnn.RNNParamsSaveable(self._cell,\n",
    "                        self._cell.params_to_canonical,\n",
    "                        self._cell.canonical_to_params, rnn_params,\n",
    "                        base_variable_scope='Model/RNN')\n",
    "                tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS,\n",
    "                        params_saveable)\n",
    "        self._cost = tf.get_collection_ref(util.with_prefix(self._name,\n",
    "                'cost'))[0]\n",
    "        num_replicas = (FLAGS.num_gpus if self._name == 'Train' else 1)\n",
    "        self._initial_state = \\\n",
    "            util.import_state_tuples(self._initial_state,\n",
    "                self._initial_state_name, num_replicas)\n",
    "        self._final_state = util.import_state_tuples(self._final_state,\n",
    "                self._final_state_name, num_replicas)\n",
    "\n",
    "    \n",
    "    # Some useful variables of the RNN\n",
    "    \n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def initial_state_name(self):\n",
    "        return self._initial_state_name\n",
    "\n",
    "    @property\n",
    "    def final_state_name(self):\n",
    "        return self._final_state_name\n",
    "    \n",
    "    @property\n",
    "    def output_probs(self):\n",
    "        return self._output_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring our Model\n",
    "\n",
    "That is ways in which we want to train this model - how long (epochs), how fast (learning rate) and many other tweaks of the hyper-parameters. It's generally noted that the longer you train a model (or the larger the config in this case) the better it performs.\n",
    "\n",
    "There are 3 supported model configurations:\n",
    "\n",
    "| config | epochs | train | valid  | test\n",
    "|-|-|-|-|-|\n",
    "| small  | 13     | 37.99 | 121.39 | 115.91\n",
    "| medium | 39     | 48.45 |  86.16 |  82.07\n",
    "| large  | 55     | 37.87 |  82.62 |  78.29\n",
    "\n",
    "\n",
    "The hyperparameters used in this model:\n",
    "- init_scale - the initial scale of the weights\n",
    "- learning_rate - the initial value of the learning rate\n",
    "- max_grad_norm - the maximum permissible norm of the gradient\n",
    "- num_layers - the number of LSTM layers\n",
    "- num_steps - the number of unrolled steps of LSTM\n",
    "- hidden_size - the number of LSTM units\n",
    "- max_epoch - the number of epochs trained with the initial learning rate\n",
    "- max_max_epoch - the total number of epochs for training\n",
    "- keep_prob - the probability of keeping weights in the dropout layer\n",
    "- lr_decay - the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "- batch_size - the batch size\n",
    "- rnn_mode - the low level implementation of lstm cell: one of CUDNN, BASIC, or BLOCK, representing cudnn_lstm, basic_lstm, and lstm_block_cell classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "\n",
    "\n",
    "class MediumConfig(object):\n",
    "    \"\"\"Medium config.\"\"\"\n",
    "    init_scale = 0.05\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 650\n",
    "    max_epoch = 6\n",
    "    max_max_epoch = 39\n",
    "    keep_prob = 0.5\n",
    "    lr_decay = 0.8\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "\n",
    "\n",
    "class LargeConfig(object):\n",
    "    \"\"\"Large config.\"\"\"\n",
    "    init_scale = 0.04\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 10\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 1500\n",
    "    max_epoch = 14\n",
    "    max_max_epoch = 55\n",
    "    keep_prob = 0.35\n",
    "    lr_decay = 1 / 1.15\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "\n",
    "\n",
    "class TestConfig(object):\n",
    "    \"\"\"Tiny config, for testing.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 1\n",
    "    num_layers = 1\n",
    "    num_steps = 2\n",
    "    hidden_size = 2\n",
    "    max_epoch = 1\n",
    "    max_max_epoch = 1\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method provides some information about the model as it trains\n",
    "def run_epoch(\n",
    "    session,\n",
    "    model,\n",
    "    eval_op=None,\n",
    "    verbose=False,\n",
    "    ):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    fetches = {'cost': model.cost, 'final_state': model.final_state}\n",
    "    if eval_op is not None:\n",
    "        fetches['eval_op'] = eval_op\n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        for (i, (c, h)) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals['cost']\n",
    "        state = vals['final_state']\n",
    "\n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "\n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "            print('%.3f perplexity: %.3f speed: %.0f wps' % (step * 1.0\n",
    "                  / model.input.epoch_size, np.exp(costs / iters),\n",
    "                  iters * model.input.batch_size * max(1,\n",
    "                  FLAGS.num_gpus) / (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters)\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    \"\"\"Get model config.\"\"\"\n",
    "\n",
    "    config = None\n",
    "    if FLAGS.model == 'small':\n",
    "        config = SmallConfig()\n",
    "    elif FLAGS.model == 'medium':\n",
    "        config = MediumConfig()\n",
    "    elif FLAGS.model == 'large':\n",
    "        config = LargeConfig()\n",
    "    elif FLAGS.model == 'test':\n",
    "        config = TestConfig()\n",
    "    elif FLAGS.model == 'generate':\n",
    "        config = SmallGenConfig()\n",
    "    else:\n",
    "        raise ValueError('Invalid model: %s', FLAGS.model)\n",
    "    if FLAGS.rnn_mode:\n",
    "        config.rnn_mode = FLAGS.rnn_mode\n",
    "    if FLAGS.num_gpus != 1 or tf.__version__ < '1.3.0':\n",
    "        config.rnn_mode = BASIC\n",
    "    return config\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    if not FLAGS.data_path:\n",
    "        raise ValueError(\"Must set --data_path to PTB data directory\")\n",
    "    gpus = [x.name for x in device_lib.list_local_devices()\n",
    "            if x.device_type == 'GPU']\n",
    "    if FLAGS.num_gpus > len(gpus):\n",
    "        raise ValueError('Your machine has only %d gpus which is less than the requested --num_gpus=%d.'\n",
    "                          % (len(gpus), FLAGS.num_gpus))\n",
    "\n",
    "    raw_data = ptb_raw_data(FLAGS.data_path)\n",
    "    (train_data, valid_data, test_data, _) = raw_data\n",
    "\n",
    "    config = get_config()\n",
    "    eval_config = get_config()\n",
    "    eval_config.batch_size = 1\n",
    "    eval_config.num_steps = 1\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                config.init_scale)\n",
    "\n",
    "        with tf.name_scope('Train'):\n",
    "            train_input = PTBInput(config=config, data=train_data,\n",
    "                                   name='TrainInput')\n",
    "            with tf.variable_scope('Model', reuse=None,\n",
    "                                   initializer=initializer):\n",
    "                m = PTBModel(is_training=True, config=config,\n",
    "                             input_=train_input)\n",
    "            tf.summary.scalar('Training Loss', m.cost)\n",
    "            tf.summary.scalar('Learning Rate', m.lr)\n",
    "\n",
    "        with tf.name_scope('Valid'):\n",
    "            valid_input = PTBInput(config=config, data=valid_data,\n",
    "                                   name='ValidInput')\n",
    "            with tf.variable_scope('Model', reuse=True,\n",
    "                                   initializer=initializer):\n",
    "                mvalid = PTBModel(is_training=False, config=config,\n",
    "                                  input_=valid_input)\n",
    "            tf.summary.scalar('Validation Loss', mvalid.cost)\n",
    "\n",
    "        with tf.name_scope('Test'):\n",
    "            test_input = PTBInput(config=eval_config, data=test_data,\n",
    "                                  name='TestInput')\n",
    "            with tf.variable_scope('Model', reuse=True,\n",
    "                                   initializer=initializer):\n",
    "                mtest = PTBModel(is_training=False, config=eval_config,\n",
    "                                 input_=test_input)\n",
    "\n",
    "        models = {'Train': m, 'Valid': mvalid, 'Test': mtest}\n",
    "        for (name, model) in models.items():\n",
    "            model.export_ops(name)\n",
    "        metagraph = tf.train.export_meta_graph()\n",
    "        if tf.__version__ < '1.1.0' and FLAGS.num_gpus > 1:\n",
    "            raise ValueError('num_gpus > 1 is not supported for TensorFlow versions below 1.1.0'\n",
    "                             )\n",
    "        soft_placement = False\n",
    "        if FLAGS.num_gpus > 1:\n",
    "            soft_placement = True\n",
    "            util.auto_parallel(metagraph, m)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        tf.train.import_meta_graph(metagraph)\n",
    "        for model in models.values():\n",
    "            model.import_ops()\n",
    "        sv = tf.train.Supervisor(logdir=FLAGS.save_path)\n",
    "        config_proto = \\\n",
    "            tf.ConfigProto(allow_soft_placement=soft_placement)\n",
    "        with sv.managed_session(config=config_proto) as session:\n",
    "            for i in range(config.max_max_epoch):\n",
    "                lr_decay = config.lr_decay ** max(i + 1\n",
    "                        - config.max_epoch, 0.0)\n",
    "                m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "                print('Epoch: %d Learning rate: %.3f' % (i + 1,\n",
    "                      session.run(m.lr)))\n",
    "                train_perplexity = run_epoch(session, m,\n",
    "                        eval_op=m.train_op, verbose=True)\n",
    "                print('Epoch: %d Train Perplexity: %.3f' % (i + 1,\n",
    "                      train_perplexity))\n",
    "                valid_perplexity = run_epoch(session, mvalid)\n",
    "                print('Epoch: %d Valid Perplexity: %.3f' % (i + 1,\n",
    "                      valid_perplexity))\n",
    "                \n",
    "            test_perplexity = run_epoch(session, mtest)\n",
    "            print('Test Perplexity: %.3f' % test_perplexity)\n",
    "\n",
    "            if FLAGS.save_path:\n",
    "                print('Saving model to %s.' % FLAGS.save_path)\n",
    "                sv.saver.save(session, FLAGS.save_path,\n",
    "                              global_step=sv.global_step)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-c6b98a0bc860>:127: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Restoring parameters from save/checkpoints/model.ckpt-730762\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path save/checkpoints/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 730762.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "0.000 perplexity: 1081.185 speed: 375 wps\n",
      "0.100 perplexity: 794.667 speed: 10265 wps\n",
      "INFO:tensorflow:Recording summary at step 761221.\n",
      "0.200 perplexity: 767.041 speed: 10258 wps\n",
      "INFO:tensorflow:Model/global_step/sec: 256.27\n",
      "0.300 perplexity: 764.183 speed: 10250 wps\n",
      "0.400 perplexity: 759.040 speed: 10349 wps\n",
      "INFO:tensorflow:Recording summary at step 792531.\n",
      "INFO:tensorflow:Model/global_step/sec: 260.958\n",
      "0.500 perplexity: 753.157 speed: 10371 wps\n",
      "0.600 perplexity: 745.739 speed: 10345 wps\n",
      "INFO:tensorflow:Recording summary at step 823565.\n",
      "INFO:tensorflow:Model/global_step/sec: 258.633\n",
      "0.700 perplexity: 739.338 speed: 10365 wps\n",
      "0.800 perplexity: 733.255 speed: 10381 wps\n",
      "INFO:tensorflow:Recording summary at step 854981.\n",
      "INFO:tensorflow:Model/global_step/sec: 261.708\n",
      "0.900 perplexity: 727.155 speed: 10375 wps\n",
      "Epoch: 1 Train Perplexity: 722.278\n",
      "INFO:tensorflow:Saving checkpoint to path save/checkpoints/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 238.558\n",
      "Epoch: 1 Valid Perplexity: 685.273\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Saving checkpoint to path save/checkpoints/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "Test Perplexity: 686.749\n",
      "Saving model to save/checkpoints/.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def start_training():\n",
    "    FLAGS.model = \"test\"\n",
    "    FLAGS.data_path = \"rnn_data\"\n",
    "    FLAGS.save_path = \"save/checkpoints/\"\n",
    "    FLAGS.use_fp16 = False\n",
    "    FLAGS.num_gpus = 0\n",
    "    FLAGS.rnn_mode = BLOCK\n",
    "    tf.app.run()\n",
    "    \n",
    "start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, our model doesn't seem to like jupyter. So I took everythin out of here and put it into a python file and ran it from terminal. Look at `/python_rnn/python_rnn.py` for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text Using Our RNN\n",
    "\n",
    "First we create a class which represents a single timestep for text generation.\n",
    "\n",
    "Then, we write a generate text which uses the saved model weights and feed-forward the input seed to receive a set of probabilities of what should come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallGenConfig(object):\n",
    "    \"\"\"Small config. for generation\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 1\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 1\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_generating():\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "    FLAGS.model = \"generating\"\n",
    "    FLAGS.data_path = \"rnn_data\"\n",
    "    FLAGS.save_path = \"save/checkpoints/\"\n",
    "    FLAGS.use_fp16 = False\n",
    "    FLAGS.num_gpus = 0\n",
    "    FLAGS.rnn_mode = BLOCK\n",
    "    \n",
    "\n",
    "# Now we set up the RNN for generation\n",
    "def generate_text(train_path, model_path, num_sentences):\n",
    "    setup_generating()\n",
    "    gen_config = SmallGenConfig()\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        initializer = tf.random_uniform_initializer(-gen_config.init_scale,\n",
    "                                                    gen_config.init_scale)    \n",
    "        with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "            m = PTBModel(is_training=False, config=gen_config, input_=PTBInput(config=gen_config, data=[[2]]))\n",
    "\n",
    "        # Restore tensorflow variables from disk.\n",
    "        saver = tf.train.Saver() \n",
    "        saver.restore(session, model_path)\n",
    "        print(\"Model restored from file \" + model_path)\n",
    "        \n",
    "    words = get_vocab(train_path)\n",
    "    \n",
    "    # We give it an initial state so it can predict what should come next\n",
    "    state = m.initial_state.eval()\n",
    "    x = 2 # the id for '<eos>' from the training set\n",
    "    gen_input = np.matrix([[x]])  # a 2D numpy matrix \n",
    "    \n",
    "    \n",
    "    # Then we let it go!\n",
    "    text = \"\"\n",
    "    count = 0\n",
    "    while count < num_sentences:\n",
    "        output_probs, state = session.run([m.output_probs, m.final_state],\n",
    "                                   {m.input_data: gen_input,\n",
    "                                    m.initial_state: state})\n",
    "        x = sample(output_probs[0], 0.9)\n",
    "        \n",
    "        if words[x]==\"<eos>\":\n",
    "            text += \".\\n\\n\"\n",
    "            count += 1\n",
    "        else:\n",
    "            text += \" \" + words[x]\n",
    "            \n",
    "        # now feed this new word as input into the next iteration\n",
    "        gen_input = np.matrix([[x]]) \n",
    "        \n",
    "    print(text)\n",
    "    return\n",
    "\n",
    "def sample(a, temperature=1.0):\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    r = random.random() # range: [0,1)\n",
    "    total = 0.0\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        total += a[i]\n",
    "        if total>r:\n",
    "            return i\n",
    "        \n",
    "    return len(a)-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"rnn_data/train.txt\", \"save/checkpoints/model.ckpt\", 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
