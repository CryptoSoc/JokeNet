{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JokeNet\n",
    "This notebook outlines a Recurrent Neural Net using LSTM cells to make jokes.\n",
    "\n",
    "For a comprehensive intro to using Recurrent Neural Nets and a few fun examples, look at this blog post by Andrei Karpathy - Tesla's AI lead. https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "The training data is an assortment from reddit, wocka.com and stupidstuff.org.\n",
    "\n",
    "It's also worthy to note that most of this code comes from the [tensorflow tutorial on RNN's](https://www.tensorflow.org/tutorials/recurrent) and is based off of the code in tf_models.\n",
    "\n",
    "For further information about the algorithm and its implementation, see here:\n",
    "> (Zaremba, et. al.) Recurrent Neural Network Regularization\n",
    "> http://arxiv.org/abs/1409.2329\n",
    "\n",
    "\n",
    "Disclaimer: these jokes are scraped from the internet and so some may be offensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import util\n",
    "\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Re)Scrape the Dataset\n",
    "\n",
    "Scraping the dataset occurs in the `jokes_dataset` folder. Re-run the scraping with the commands:\n",
    "```\n",
    "python reddit.py\n",
    "python wocka.py\n",
    "python stupidstuff.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_joke(id):\n",
    "    \"\"\"Download and parse a single joke.\"\"\"\n",
    "    \n",
    "    re_category_rating = re.compile(r\"\\s*Category: (.*[A-z])\\s*Rating: (.*\\d)\\s*\")\n",
    "\n",
    "    url_base = \"http://stupidstuff.org/jokes/joke.htm?jokeid={}\"\n",
    "    response = requests.get(url_base.format(id))\n",
    "\n",
    "    tree = html.fromstring(response.content)\n",
    "    content = tree.xpath('//table[@bgcolor=\"#ffffff\" and @width=\"470\"]//table[@class=\"scroll\"]//td')[0]\n",
    "    category_rating_cells = content.xpath('//table[@bgcolor=\"#ffffff\"]//table[@class=\"bkline\"]//td/b[text()=\"Category: \"]/..')\n",
    "    #print(category_rating_cell)\n",
    "    \n",
    "    # all html nodes in content, but not plaintext\n",
    "    crap = content.xpath('./child::node()[not(self::text()) and not(self::br)]') \n",
    "    \n",
    "    for node in crap:\n",
    "        content.remove(node)\n",
    "\n",
    "    body_text = content.text_content().strip()\n",
    "    joke_body = body_text\n",
    "\n",
    "    cell_text = category_rating_cells[0].text_content()\n",
    "\n",
    "    match = re_category_rating.search(cell_text)\n",
    "    category = match.group(1)\n",
    "    rating = float(match.group(2))\n",
    "\n",
    "    return joke_body, category, rating\n",
    "\n",
    "\n",
    "def save_stupid_stuff_jokes():\n",
    "    \"\"\"Parse and save all jokes to stupidstuff.json file\"\"\"\n",
    "    jokes = []\n",
    "\n",
    "    save_frequency = 100 # save after every 100 IDs\n",
    "    max_id = 3773\n",
    "    for id in range(1, max_id+1): #19000\n",
    "        try:\n",
    "            body, category, rating = extract_joke(id)\n",
    "\n",
    "            joke = {\"id\": id, \"category\": category, \"body\": body, \"rating\": rating}\n",
    "            jokes.append(joke)\n",
    "            print(\"ID {} success: [{}]\".format(id, category))\n",
    "        except Exception as ex:\n",
    "            print(\"ID {} failed: \".format(id))\n",
    "            logging.error(ex)\n",
    "            raise ex\n",
    "\n",
    "        if id % save_frequency == 0 or id == max_id:\n",
    "            with open(\"stupidstuff.json\", \"w\") as f:\n",
    "                json.dump(jokes, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing functions\n",
    "\n",
    "The data is formatted into json objects with various IDs:\n",
    "    \n",
    "reddit_jokes.json\n",
    "```json\n",
    "{\n",
    "    \"title\": \"My boss said to me, \\\"you're the worst train driver ever. How many have you derailed this year?\\\"\",\n",
    "    \"body\": \"I said, \\\"I'm not sure; it's hard to keep track.\\\"\",\n",
    "    \"id\": \"5tyytx\",\n",
    "    \"score\": 3\n",
    "}\n",
    "```\n",
    "\n",
    "stupidstuff.json\n",
    "```json\n",
    "{\n",
    "    \"category\": \"Blonde Jokes\",\n",
    "    \"body\": \"A blonde is walking down the street with her blouse open, exposing one of her breasts. A nearby policeman approaches her and remarks, \\\"Ma'am, are you aware that I could cite you for indecent exposure?\\\" \\\"Why, officer?\\\" asks the blonde. \\\"Because your blouse is open and your breast is exposed.\\\" \\\"Oh my goodness,\\\" exclaims the blonde, \\\"I must have left my baby on the bus!\\\"\",\n",
    "    \"id\": 14,\n",
    "    \"rating\": 3.5\n",
    "}\n",
    "```\n",
    "\n",
    "wocka.json\n",
    "```json\n",
    "{\n",
    "    \"title\": \"Infants vs Adults\",\n",
    "    \"body\": \"Do infants enjoy infancy as much as adults enjoy adultery?\",\n",
    "    \"category\": \"One Liners\",\n",
    "    \"id\": 17\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "The data must be processed into training, testing and validation sets. It also must be shuffled around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import os\n",
    "\n",
    "\n",
    "def read_reddit(filename):\n",
    "    '''Parse reddit jokes which often include the title into the joke.'''\n",
    "    jokes = []\n",
    "\n",
    "    with open(\"rnn_data/reddit_jokes.json\",\"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    for joke in data:\n",
    "        jokes.append(joke[\"title\"] + \"\\n\" + joke[\"body\"])\n",
    "    \n",
    "    return jokes\n",
    "\n",
    "def read_other(filename):\n",
    "    '''Parse other jokes from stupidstuff or wocka'''\n",
    "    jokes = []\n",
    "\n",
    "    with open(filename,\"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    for joke in data:\n",
    "        jokes.append(joke[\"body\"])\n",
    "    \n",
    "    return jokes\n",
    "\n",
    "\n",
    "def parse_raw_data(data_path=\"\"):\n",
    "    \"\"\"Collate data objects and randomly shuffle into train, test, validate splits\"\"\"\n",
    "    \n",
    "    jokes = []\n",
    "\n",
    "    jokes += read_reddit(os.path.join(data_path, \"reddit_jokes.json\"))\n",
    "    jokes += read_other(os.path.join(data_path, \"wocka.json\"))\n",
    "    jokes += read_other(os.path.join(data_path, \"stupidstuff.json\"))\n",
    "\n",
    "    shuffle(jokes)\n",
    "    \n",
    "    jokes_length = len(jokes)\n",
    "    train, test, validate = jokes[0:int(0.7*jokes_length)], \\\n",
    "                            jokes[int(0.7*jokes_length)+1:int(0.85*jokes_length)], \\\n",
    "                            jokes[int(0.85*jokes_length)+1:]\n",
    "\n",
    "    with open(\"rnn_data/train.txt\",\"w\") as f:\n",
    "        for line in train:\n",
    "            f.write(line + \"\\n\\n\")\n",
    "    \n",
    "    with open(\"rnn_data/test.txt\",\"w\") as f:\n",
    "        for line in test:\n",
    "            f.write(line + \"\\n\\n\")\n",
    "            \n",
    "    with open(\"rnn_data/validate.txt\",\"w\") as f:\n",
    "        for line in validate:\n",
    "            f.write(line + \"\\n\\n\")\n",
    "    \n",
    "            \n",
    "# parse_raw_data(data_path=\"rnn_data\") # uncomment if you want to reshuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further processing\n",
    "\n",
    "The data is then turned into lists of numbers representing the words. These numbers can then be trained on by the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import sys\n",
    "\n",
    "\n",
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\\n\", \"<eos>\").replace(\"\\r\", \"\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename, maxlen=-1):\n",
    "    data = _read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words[0:maxlen], range(len(words[0:maxlen]))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def get_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "    \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "\n",
    "    Reads PTB text files, converts strings to integer ids,\n",
    "    and performs mini-batching of the inputs.\n",
    "\n",
    "    The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "\n",
    "    http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "\n",
    "    Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "\n",
    "    Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "    \"\"\"\n",
    "\n",
    "    train_path = os.path.join(data_path, \"train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"test.txt\")\n",
    "    test_path = os.path.join(data_path, \"validate.txt\")\n",
    "\n",
    "    word_to_id = _build_vocab(train_path, 9999)\n",
    "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "\n",
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "    \"\"\"Iterate on the raw PTB data.\n",
    "\n",
    "    This chunks up raw_data into batches of examples and returns Tensors that\n",
    "    are drawn from these batches.\n",
    "\n",
    "    Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    name: the name of this operation (optional).\n",
    "\n",
    "    Returns:\n",
    "    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "    of the tuple is the same data time-shifted to the right by one.\n",
    "\n",
    "    Raises:\n",
    "    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "        raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    assertion = tf.assert_positive(\n",
    "        epoch_size,\n",
    "        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    with tf.control_dependencies([assertion]):\n",
    "        epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = tf.strided_slice(data, [0, i * num_steps],\n",
    "                         [batch_size, (i + 1) * num_steps])\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = tf.strided_slice(data, [0, i * num_steps + 1],\n",
    "                         [batch_size, (i + 1) * num_steps + 1])\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the RNN Model\n",
    "\n",
    "Tensorflow wraps up the math for us but we have to look at a multitude of `tf.this.that.theother`. Don't let this scare you though - in fact the entirety of a neural network could be written in [11 lines of python without tensorflow](https://iamtrask.github.io/2015/07/12/basic-python-network/). Tensorflow allows us to do fancier things faster - at least thats the idea.\n",
    "\n",
    "There are 3 supported model configurations:\n",
    "\n",
    "| config | epochs | train | valid  | test\n",
    "|-|-|-|-|-|\n",
    "| small  | 13     | 37.99 | 121.39 | 115.91\n",
    "| medium | 39     | 48.45 |  86.16 |  82.07\n",
    "| large  | 55     | 37.87 |  82.62 |  78.29\n",
    "\n",
    "That is ways in which we want to train this model - how long (epochs), how fast (learning rate) and many other tweaks of the hyper-parameters. It's generally noted that the longer you train a model (or the larger the config in this case) the better it performs.\n",
    "\n",
    "The hyperparameters used in this model:\n",
    "- init_scale - the initial scale of the weights\n",
    "- learning_rate - the initial value of the learning rate\n",
    "- max_grad_norm - the maximum permissible norm of the gradient\n",
    "- num_layers - the number of LSTM layers\n",
    "- num_steps - the number of unrolled steps of LSTM\n",
    "- hidden_size - the number of LSTM units\n",
    "- max_epoch - the number of epochs trained with the initial learning rate\n",
    "- max_max_epoch - the total number of epochs for training\n",
    "- keep_prob - the probability of keeping weights in the dropout layer\n",
    "- lr_decay - the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "- batch_size - the batch size\n",
    "- rnn_mode - the low level implementation of lstm cell: one of CUDNN, BASIC, or BLOCK, representing cudnn_lstm, basic_lstm, and lstm_block_cell classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "flags.DEFINE_string('model', 'small',\n",
    "                    'A type of model. Possible options are: small, medium, large.'\n",
    "                    )\n",
    "flags.DEFINE_string('data_path', None,\n",
    "                    'Where the training/test data is stored.')\n",
    "flags.DEFINE_string('save_path', None, 'Model output directory.')\n",
    "flags.DEFINE_bool('use_fp16', False,\n",
    "                  'Train using 16-bit floats instead of 32bit floats')\n",
    "flags.DEFINE_integer('num_gpus', 1,\n",
    "                     'If larger than 1, Grappler AutoParallel optimizer will create multiple training replicas with each GPU running one replica.'\n",
    "                     )\n",
    "\n",
    "flags.DEFINE_string('rnn_mode', None,\n",
    "                    'The low level implementation of lstm cell: one of CUDNN, BASIC, and BLOCK, representing cudnn_lstm, basic_lstm, and lstm_block_cell classes.'\n",
    "                    )\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "BASIC = 'basic'\n",
    "CUDNN = 'cudnn'\n",
    "BLOCK = 'block'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_type():\n",
    "    return tf.float32 #(tf.float16 if FLAGS.use_fp16 else tf.float32)\n",
    "\n",
    "\n",
    "class PTBInput(object):\n",
    "\n",
    "    \"\"\"The input data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        data,\n",
    "        name=None,\n",
    "        ):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = (len(data) // batch_size - 1) // num_steps\n",
    "        (self.input_data, self.targets) = ptb_producer(data,\n",
    "                batch_size, num_steps, name=name)\n",
    "\n",
    "\n",
    "class PTBModel(object):\n",
    "\n",
    "    \"\"\"The PTB model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_training,\n",
    "        config,\n",
    "        input_,\n",
    "        ):\n",
    "        self._is_training = is_training\n",
    "        self._input = input_\n",
    "        self._rnn_params = None\n",
    "        self._cell = None\n",
    "        self.batch_size = input_.batch_size\n",
    "        self.num_steps = input_.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [vocab_size,\n",
    "                    size], dtype=tf.float32) #data_type()\n",
    "            inputs = tf.nn.embedding_lookup(embedding,\n",
    "                    input_.input_data)\n",
    "\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        (output, state) = self._build_rnn_graph(inputs, config,\n",
    "                is_training)\n",
    "\n",
    "        softmax_w = tf.get_variable('softmax_w', [size, vocab_size],\n",
    "                                    dtype=tf.float32) #data_type()\n",
    "        softmax_b = tf.get_variable('softmax_b', [vocab_size],\n",
    "                                    dtype=tf.float32) #data_type()\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "\n",
    "     # Reshape logits to be a 3-D tensor for sequence loss\n",
    "\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps,\n",
    "                            vocab_size])\n",
    "        \n",
    "        self._output_probs = tf.nn.softmax(logits)\n",
    "\n",
    "    # Use the contrib sequence loss and average over the batches\n",
    "\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits, input_.targets,\n",
    "                tf.ones([self.batch_size, self.num_steps],\n",
    "                dtype=data_type()), average_across_timesteps=False,\n",
    "                average_across_batch=True)\n",
    "\n",
    "    # Update the cost\n",
    "\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        (grads, _) = tf.clip_by_global_norm(tf.gradients(self._cost,\n",
    "                tvars), config.max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(tf.float32, shape=[],\n",
    "                name='new_learning_rate')\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "\n",
    "    def _build_rnn_graph(\n",
    "        self,\n",
    "        inputs,\n",
    "        config,\n",
    "        is_training,\n",
    "        ):\n",
    "        if config.rnn_mode == CUDNN:\n",
    "            return self._build_rnn_graph_cudnn(inputs, config,\n",
    "                    is_training)\n",
    "        else:\n",
    "            return self._build_rnn_graph_lstm(inputs, config,\n",
    "                    is_training)\n",
    "\n",
    "    def _build_rnn_graph_cudnn(\n",
    "        self,\n",
    "        inputs,\n",
    "        config,\n",
    "        is_training,\n",
    "        ):\n",
    "        \"\"\"Build the inference graph using CUDNN cell.\"\"\"\n",
    "\n",
    "        inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "        self._cell = \\\n",
    "            tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=config.num_layers,\n",
    "                num_units=config.hidden_size,\n",
    "                input_size=config.hidden_size, dropout=(1\n",
    "                - config.keep_prob if is_training else 0))\n",
    "        params_size_t = self._cell.params_size()\n",
    "        self._rnn_params = tf.get_variable('lstm_params',\n",
    "                initializer=tf.random_uniform([params_size_t],\n",
    "                -config.init_scale, config.init_scale),\n",
    "                validate_shape=False)\n",
    "        c = tf.zeros([config.num_layers, self.batch_size,\n",
    "                     config.hidden_size], tf.float32)\n",
    "        h = tf.zeros([config.num_layers, self.batch_size,\n",
    "                     config.hidden_size], tf.float32)\n",
    "        self._initial_state = (tf.contrib.rnn.LSTMStateTuple(h=h, c=c),\n",
    "                               )\n",
    "        (outputs, h, c) = self._cell(inputs, h, c, self._rnn_params,\n",
    "                is_training)\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
    "        return (outputs, (tf.contrib.rnn.LSTMStateTuple(h=h, c=c), ))\n",
    "\n",
    "    def _get_lstm_cell(self, config, is_training):\n",
    "        if config.rnn_mode == BASIC:\n",
    "            return tf.contrib.rnn.BasicLSTMCell(config.hidden_size,\n",
    "                    forget_bias=0.0, state_is_tuple=True,\n",
    "                    reuse=not is_training)\n",
    "        if config.rnn_mode == BLOCK:\n",
    "            return tf.contrib.rnn.LSTMBlockCell(config.hidden_size,\n",
    "                    forget_bias=0.0)\n",
    "        raise ValueError('rnn_mode %s not supported' % config.rnn_mode)\n",
    "\n",
    "    def _build_rnn_graph_lstm(\n",
    "        self,\n",
    "        inputs,\n",
    "        config,\n",
    "        is_training,\n",
    "        ):\n",
    "        \"\"\"Build the inference graph using canonical LSTM cells.\"\"\"\n",
    "\n",
    "    # Slightly better results can be obtained with forget gate biases\n",
    "    # initialized to 1 but the hyperparameters of the model would need to be\n",
    "    # different than reported in the paper.\n",
    "\n",
    "        def make_cell():\n",
    "            cell = self._get_lstm_cell(config, is_training)\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                        output_keep_prob=config.keep_prob)\n",
    "            return cell\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([make_cell() for _ in\n",
    "                range(config.num_layers)], state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(config.batch_size,\n",
    "                data_type())\n",
    "        state = self._initial_state\n",
    "\n",
    "    # Simplified version of tf.nn.static_rnn().\n",
    "    # This builds an unrolled LSTM for tutorial purposes only.\n",
    "    # In general, use tf.nn.static_rnn() or tf.nn.static_state_saving_rnn().\n",
    "    #\n",
    "    # The alternative version of the code below is:\n",
    "    #\n",
    "    # inputs = tf.unstack(inputs, num=self.num_steps, axis=1)\n",
    "    # outputs, state = tf.nn.static_rnn(cell, inputs,\n",
    "    #                                   initial_state=self._initial_state)\n",
    "\n",
    "        outputs = []\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for time_step in range(self.num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(inputs[:, time_step, :],\n",
    "                        state)\n",
    "                outputs.append(cell_output)\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1,\n",
    "                            config.hidden_size])\n",
    "        return (output, state)\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "    def export_ops(self, name):\n",
    "        \"\"\"Exports ops to collections.\"\"\"\n",
    "\n",
    "        self._name = name\n",
    "        ops = {util.with_prefix(self._name, 'cost'): self._cost}\n",
    "        if self._is_training:\n",
    "            ops.update(lr=self._lr, new_lr=self._new_lr,\n",
    "                       lr_update=self._lr_update)\n",
    "            if self._rnn_params:\n",
    "                ops.update(rnn_params=self._rnn_params)\n",
    "        for (name, op) in ops.items():\n",
    "            tf.add_to_collection(name, op)\n",
    "        self._initial_state_name = util.with_prefix(self._name,\n",
    "                'initial')\n",
    "        self._final_state_name = util.with_prefix(self._name, 'final')\n",
    "        util.export_state_tuples(self._initial_state,\n",
    "                                 self._initial_state_name)\n",
    "        util.export_state_tuples(self._final_state,\n",
    "                                 self._final_state_name)\n",
    "\n",
    "    def import_ops(self):\n",
    "        \"\"\"Imports ops from collections.\"\"\"\n",
    "\n",
    "        if self._is_training:\n",
    "            self._train_op = tf.get_collection_ref('train_op')[0]\n",
    "            self._lr = tf.get_collection_ref('lr')[0]\n",
    "            self._new_lr = tf.get_collection_ref('new_lr')[0]\n",
    "            self._lr_update = tf.get_collection_ref('lr_update')[0]\n",
    "            rnn_params = tf.get_collection_ref('rnn_params')\n",
    "            if self._cell and rnn_params:\n",
    "                params_saveable = \\\n",
    "                    tf.contrib.cudnn_rnn.RNNParamsSaveable(self._cell,\n",
    "                        self._cell.params_to_canonical,\n",
    "                        self._cell.canonical_to_params, rnn_params,\n",
    "                        base_variable_scope='Model/RNN')\n",
    "                tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS,\n",
    "                        params_saveable)\n",
    "        self._cost = tf.get_collection_ref(util.with_prefix(self._name,\n",
    "                'cost'))[0]\n",
    "        num_replicas = (FLAGS.num_gpus if self._name == 'Train' else 1)\n",
    "        self._initial_state = \\\n",
    "            util.import_state_tuples(self._initial_state,\n",
    "                self._initial_state_name, num_replicas)\n",
    "        self._final_state = util.import_state_tuples(self._final_state,\n",
    "                self._final_state_name, num_replicas)\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def initial_state_name(self):\n",
    "        return self._initial_state_name\n",
    "\n",
    "    @property\n",
    "    def final_state_name(self):\n",
    "        return self._final_state_name\n",
    "    \n",
    "    @property\n",
    "    def output_probs(self):\n",
    "        return self._output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "\n",
    "\n",
    "class MediumConfig(object):\n",
    "    \"\"\"Medium config.\"\"\"\n",
    "    init_scale = 0.05\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 650\n",
    "    max_epoch = 6\n",
    "    max_max_epoch = 39\n",
    "    keep_prob = 0.5\n",
    "    lr_decay = 0.8\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "\n",
    "\n",
    "class LargeConfig(object):\n",
    "    \"\"\"Large config.\"\"\"\n",
    "    init_scale = 0.04\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 10\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 1500\n",
    "    max_epoch = 14\n",
    "    max_max_epoch = 55\n",
    "    keep_prob = 0.35\n",
    "    lr_decay = 1 / 1.15\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "\n",
    "\n",
    "class TestConfig(object):\n",
    "    \"\"\"Tiny config, for testing.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 1\n",
    "    num_layers = 1\n",
    "    num_steps = 2\n",
    "    hidden_size = 2\n",
    "    max_epoch = 1\n",
    "    max_max_epoch = 1\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    session,\n",
    "    model,\n",
    "    eval_op=None,\n",
    "    verbose=False,\n",
    "    ):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    fetches = {'cost': model.cost, 'final_state': model.final_state}\n",
    "    if eval_op is not None:\n",
    "        fetches['eval_op'] = eval_op\n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        for (i, (c, h)) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals['cost']\n",
    "        state = vals['final_state']\n",
    "\n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "\n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "            print('%.3f perplexity: %.3f speed: %.0f wps' % (step * 1.0\n",
    "                  / model.input.epoch_size, np.exp(costs / iters),\n",
    "                  iters * model.input.batch_size * max(1,\n",
    "                  FLAGS.num_gpus) / (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters)\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    \"\"\"Get model config.\"\"\"\n",
    "\n",
    "    config = None\n",
    "    if FLAGS.model == 'small':\n",
    "        config = SmallConfig()\n",
    "    elif FLAGS.model == 'medium':\n",
    "        config = MediumConfig()\n",
    "    elif FLAGS.model == 'large':\n",
    "        config = LargeConfig()\n",
    "    elif FLAGS.model == 'test':\n",
    "        config = TestConfig()\n",
    "    elif FLAGS.model == 'generate':\n",
    "        config = SmallGenConfig()\n",
    "    else:\n",
    "        raise ValueError('Invalid model: %s', FLAGS.model)\n",
    "    if FLAGS.rnn_mode:\n",
    "        config.rnn_mode = FLAGS.rnn_mode\n",
    "    if FLAGS.num_gpus != 1 or tf.__version__ < '1.3.0':\n",
    "        config.rnn_mode = BASIC\n",
    "    return config\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    if not FLAGS.data_path:\n",
    "        raise ValueError(\"Must set --data_path to PTB data directory\")\n",
    "    gpus = [x.name for x in device_lib.list_local_devices()\n",
    "            if x.device_type == 'GPU']\n",
    "    if FLAGS.num_gpus > len(gpus):\n",
    "        raise ValueError('Your machine has only %d gpus which is less than the requested --num_gpus=%d.'\n",
    "                          % (len(gpus), FLAGS.num_gpus))\n",
    "\n",
    "    raw_data = ptb_raw_data(FLAGS.data_path)\n",
    "    (train_data, valid_data, test_data, _) = raw_data\n",
    "\n",
    "    config = get_config()\n",
    "    eval_config = get_config()\n",
    "    eval_config.batch_size = 1\n",
    "    eval_config.num_steps = 1\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                config.init_scale)\n",
    "\n",
    "        with tf.name_scope('Train'):\n",
    "            train_input = PTBInput(config=config, data=train_data,\n",
    "                                   name='TrainInput')\n",
    "            with tf.variable_scope('Model', reuse=None,\n",
    "                                   initializer=initializer):\n",
    "                m = PTBModel(is_training=True, config=config,\n",
    "                             input_=train_input)\n",
    "            tf.summary.scalar('Training Loss', m.cost)\n",
    "            tf.summary.scalar('Learning Rate', m.lr)\n",
    "\n",
    "        with tf.name_scope('Valid'):\n",
    "            valid_input = PTBInput(config=config, data=valid_data,\n",
    "                                   name='ValidInput')\n",
    "            with tf.variable_scope('Model', reuse=True,\n",
    "                                   initializer=initializer):\n",
    "                mvalid = PTBModel(is_training=False, config=config,\n",
    "                                  input_=valid_input)\n",
    "            tf.summary.scalar('Validation Loss', mvalid.cost)\n",
    "\n",
    "        with tf.name_scope('Test'):\n",
    "            test_input = PTBInput(config=eval_config, data=test_data,\n",
    "                                  name='TestInput')\n",
    "            with tf.variable_scope('Model', reuse=True,\n",
    "                                   initializer=initializer):\n",
    "                mtest = PTBModel(is_training=False, config=eval_config,\n",
    "                                 input_=test_input)\n",
    "\n",
    "        models = {'Train': m, 'Valid': mvalid, 'Test': mtest}\n",
    "        for (name, model) in models.items():\n",
    "            model.export_ops(name)\n",
    "        metagraph = tf.train.export_meta_graph()\n",
    "        if tf.__version__ < '1.1.0' and FLAGS.num_gpus > 1:\n",
    "            raise ValueError('num_gpus > 1 is not supported for TensorFlow versions below 1.1.0'\n",
    "                             )\n",
    "        soft_placement = False\n",
    "        if FLAGS.num_gpus > 1:\n",
    "            soft_placement = True\n",
    "            util.auto_parallel(metagraph, m)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        tf.train.import_meta_graph(metagraph)\n",
    "        for model in models.values():\n",
    "            model.import_ops()\n",
    "        sv = tf.train.Supervisor(logdir=FLAGS.save_path)\n",
    "        config_proto = \\\n",
    "            tf.ConfigProto(allow_soft_placement=soft_placement)\n",
    "        with sv.managed_session(config=config_proto) as session:\n",
    "            for i in range(config.max_max_epoch):\n",
    "                lr_decay = config.lr_decay ** max(i + 1\n",
    "                        - config.max_epoch, 0.0)\n",
    "                m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "                print('Epoch: %d Learning rate: %.3f' % (i + 1,\n",
    "                      session.run(m.lr)))\n",
    "                train_perplexity = run_epoch(session, m,\n",
    "                        eval_op=m.train_op, verbose=True)\n",
    "                print('Epoch: %d Train Perplexity: %.3f' % (i + 1,\n",
    "                      train_perplexity))\n",
    "                valid_perplexity = run_epoch(session, mvalid)\n",
    "                print('Epoch: %d Valid Perplexity: %.3f' % (i + 1,\n",
    "                      valid_perplexity))\n",
    "                \n",
    "            test_perplexity = run_epoch(session, mtest)\n",
    "            print('Test Perplexity: %.3f' % test_perplexity)\n",
    "\n",
    "            if FLAGS.save_path:\n",
    "                print('Saving model to %s.' % FLAGS.save_path)\n",
    "                sv.saver.save(session, FLAGS.save_path,\n",
    "                              global_step=sv.global_step)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "INFO:tensorflow:Summary name Learning Rate is illegal; using Learning_Rate instead.\n",
      "INFO:tensorflow:Summary name Validation Loss is illegal; using Validation_Loss instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-c6b98a0bc860>:127: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Restoring parameters from save/checkpoints/model.ckpt-730762\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path save/checkpoints/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 730762.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "0.000 perplexity: 1081.185 speed: 375 wps\n",
      "0.100 perplexity: 794.667 speed: 10265 wps\n",
      "INFO:tensorflow:Recording summary at step 761221.\n",
      "0.200 perplexity: 767.041 speed: 10258 wps\n",
      "INFO:tensorflow:Model/global_step/sec: 256.27\n",
      "0.300 perplexity: 764.183 speed: 10250 wps\n",
      "0.400 perplexity: 759.040 speed: 10349 wps\n",
      "INFO:tensorflow:Recording summary at step 792531.\n",
      "INFO:tensorflow:Model/global_step/sec: 260.958\n",
      "0.500 perplexity: 753.157 speed: 10371 wps\n",
      "0.600 perplexity: 745.739 speed: 10345 wps\n",
      "INFO:tensorflow:Recording summary at step 823565.\n",
      "INFO:tensorflow:Model/global_step/sec: 258.633\n",
      "0.700 perplexity: 739.338 speed: 10365 wps\n",
      "0.800 perplexity: 733.255 speed: 10381 wps\n",
      "INFO:tensorflow:Recording summary at step 854981.\n",
      "INFO:tensorflow:Model/global_step/sec: 261.708\n",
      "0.900 perplexity: 727.155 speed: 10375 wps\n",
      "Epoch: 1 Train Perplexity: 722.278\n",
      "INFO:tensorflow:Saving checkpoint to path save/checkpoints/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 238.558\n",
      "Epoch: 1 Valid Perplexity: 685.273\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Saving checkpoint to path save/checkpoints/model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "INFO:tensorflow:Recording summary at step 883789.\n",
      "Test Perplexity: 686.749\n",
      "Saving model to save/checkpoints/.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def start_training():\n",
    "    FLAGS.model = \"test\"\n",
    "    FLAGS.data_path = \"rnn_data\"\n",
    "    FLAGS.save_path = \"save/checkpoints/\"\n",
    "    FLAGS.use_fp16 = False\n",
    "    FLAGS.num_gpus = 0\n",
    "    FLAGS.rnn_mode = BLOCK\n",
    "    tf.app.run()\n",
    "    \n",
    "start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text Using Our RNN\n",
    "\n",
    "First we create a class which represents a single timestep for text generation.\n",
    "\n",
    "Then, we write a generate text which uses the saved model weights and feed-forward the input seed to receive a set of probabilities of what should come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallGenConfig(object):\n",
    "    \"\"\"Small config. for generation\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 1\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 1\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_generating():\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "    FLAGS.model = \"generating\"\n",
    "    FLAGS.data_path = \"rnn_data\"\n",
    "    FLAGS.save_path = \"save/checkpoints/\"\n",
    "    FLAGS.use_fp16 = False\n",
    "    FLAGS.num_gpus = 0\n",
    "    FLAGS.rnn_mode = BLOCK\n",
    "    \n",
    "\n",
    "\n",
    "def generate_text(train_path, model_path, num_sentences):\n",
    "    setup_generating()\n",
    "    gen_config = SmallGenConfig()\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "        initializer = tf.random_uniform_initializer(-gen_config.init_scale,\n",
    "                                                    gen_config.init_scale)    \n",
    "        with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "            m = PTBModel(is_training=False, config=gen_config, input_=PTBInput(config=gen_config, data=[[2]]))\n",
    "\n",
    "        # Restore variables from disk.\n",
    "        saver = tf.train.Saver() \n",
    "        saver.restore(session, model_path)\n",
    "        print(\"Model restored from file \" + model_path)\n",
    "        \n",
    "    words = get_vocab(train_path)\n",
    "    \n",
    "    state = m.initial_state.eval()\n",
    "    x = 2 # the id for '<eos>' from the training set\n",
    "    gen_input = np.matrix([[x]])  # a 2D numpy matrix \n",
    "    \n",
    "    text = \"\"\n",
    "    count = 0\n",
    "    while count < num_sentences:\n",
    "        output_probs, state = session.run([m.output_probs, m.final_state],\n",
    "                                   {m.input_data: gen_input,\n",
    "                                    m.initial_state: state})\n",
    "        x = sample(output_probs[0], 0.9)\n",
    "        if words[x]==\"<eos>\":\n",
    "            text += \".\\n\\n\"\n",
    "            count += 1\n",
    "        else:\n",
    "            text += \" \" + words[x]\n",
    "            \n",
    "        # now feed this new word as input into the next iteration\n",
    "        gen_input = np.matrix([[x]]) \n",
    "        \n",
    "    print(text)\n",
    "    return\n",
    "\n",
    "def sample(a, temperature=1.0):\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    r = random.random() # range: [0,1)\n",
    "    total = 0.0\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        total += a[i]\n",
    "        if total>r:\n",
    "            return i\n",
    "        \n",
    "    return len(a)-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/checkpoints/model.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for save/checkpoints/model.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/home/tom/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/tom/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-7fba0dc79206>\", line 1, in <module>\n    generate_text(\"rnn_data/train.txt\", \"save/checkpoints/model.ckpt\", 10)\n  File \"<ipython-input-27-b400b92a0a4d>\", line 23, in generate_text\n    saver = tf.train.Saver()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for save/checkpoints/model.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for save/checkpoints/model.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7fba0dc79206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rnn_data/train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"save/checkpoints/model.ckpt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-b400b92a0a4d>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(train_path, model_path, num_sentences)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Restore variables from disk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored from file \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1800\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1802\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for save/checkpoints/model.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/home/tom/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/tom/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-7fba0dc79206>\", line 1, in <module>\n    generate_text(\"rnn_data/train.txt\", \"save/checkpoints/model.ckpt\", 10)\n  File \"<ipython-input-27-b400b92a0a4d>\", line 23, in generate_text\n    saver = tf.train.Saver()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/tom/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for save/checkpoints/model.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"rnn_data/train.txt\", \"save/checkpoints/model.ckpt\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-004ee8d0f68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-004ee8d0f68e>\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLOCK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
